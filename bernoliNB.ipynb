{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bernoliNB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farooqzaman1/DataSciencePrj/blob/master/bernoliNB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "2jvaYtLkoSuU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Quora Insincere Questions Classification\n",
        "#Detect toxic content to improve online conversations\n",
        "An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n",
        "\n",
        "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
        "\n",
        "In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.\n",
        "\n",
        "Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.\n"
      ]
    },
    {
      "metadata": {
        "id": "0bJf2RWXoEgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing required libararies**\n"
      ]
    },
    {
      "metadata": {
        "id": "yur6vbvihw9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSVc8fP8qRNs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Using google drive for project data resources**"
      ]
    },
    {
      "metadata": {
        "id": "6RhnVAU8kYqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "# files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtkqfG8rq9pj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**loading dataset**"
      ]
    },
    {
      "metadata": {
        "id": "kSCpvl7tjTg2",
        "colab_type": "code",
        "outputId": "0d011085-4e0a-45e4-8a83-1a48ba1940ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv(\"drive/My Drive/DataScienceProject/train.csv\")\n",
        "print(\"total Instances: \",data.shape[0])\n",
        "\n",
        "data=data.sort_values(by=['target'])\n",
        "\n",
        "print(\"class 1: \",data['target'].sum(), \"class 0: \", len(data['target'])-data['target'].sum())\n",
        "SampleIndex=data['target'].sum() * 2 + 1000\n",
        "\n",
        "dataSample= data[-SampleIndex:]\n",
        "print(\"Selected Subset: \",dataSample.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total Instances:  1306122\n",
            "class 1:  80810 class 0:  1225312\n",
            "Selected Subset:  162620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RsriJCCZrvMi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Droping Nan values**"
      ]
    },
    {
      "metadata": {
        "id": "lwXpNb2orgSa",
        "colab_type": "code",
        "outputId": "c1bfb0f3-f1ab-487a-f38e-3d929d2b13bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "data = data[pd.notnull(data['target'])]\n",
        "\n",
        "print(\"is there any null?\")\n",
        "print(data.isna().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is there any null?\n",
            "qid              0\n",
            "question_text    0\n",
            "target           0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vjqpg7yer75u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Splitting data into train and test part**\n"
      ]
    },
    {
      "metadata": {
        "id": "nYMYBPF4rl0L",
        "colab_type": "code",
        "outputId": "9dd4c26e-11a8-4ff4-c355-eaa175916d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(dataSample, test_size=0.3)\n",
        "\n",
        "X_train = train['question_text']\n",
        "y_train = train['target']\n",
        "\n",
        "    \n",
        "X_test = test['question_text']\n",
        "y_test = test['target']\n",
        "print(\"Training on :\", len(train['target']))\n",
        "print(\"test on: \", len(test['target']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on : 113834\n",
            "test on:  48786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ssz7z2Ftn-7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Helper Function for reading Files**"
      ]
    },
    {
      "metadata": {
        "id": "tgOcLO1vj8qD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read Files\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r', encoding=\"utf-8\")\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKZg2mN1twVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create list of valid tokens from text**"
      ]
    },
    {
      "metadata": {
        "id": "eRWTWDY1tvGu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create tokens of text, remove punctuation marks and filter out invalid tokens\n",
        "def clean_question(quest, vocab):\n",
        "\t# create tokens using white space\n",
        "\ttokens = quest.split()\n",
        "\t# remove punctuation\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjqgpIDUuAdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**process list of documents**"
      ]
    },
    {
      "metadata": {
        "id": "q5trikMIj2uu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load all docs from a directory\n",
        "def process_docs(docs, vocab,):\n",
        "    documents = list()\n",
        "    for d in docs:\n",
        "        tokens = clean_question(d, vocab)\n",
        "        documents.append(tokens)\n",
        "    return documents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clJ97ar6udoW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading vocablary previously created  on complete data**"
      ]
    },
    {
      "metadata": {
        "id": "nW0Le5b_uefd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'drive/My Drive/DataScienceProject/vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49Xt6trJutpa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**load all training reviews**"
      ]
    },
    {
      "metadata": {
        "id": "y_rkAb5TjyZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load all training Questions\n",
        "train_docs = process_docs(X_train, vocab)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tMHRTx0EvpZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create tokenizer and convert text to sequences of maximum document length**"
      ]
    },
    {
      "metadata": {
        "id": "W6KNAbD2vqMw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the document\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v_tl-EAlxO7I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Applying Bernoulli Naive Bayes from Sklearn**"
      ]
    },
    {
      "metadata": {
        "id": "gvhI_gLhjk3U",
        "colab_type": "code",
        "outputId": "f45d0c80-38db-4e3b-d585-9026c7d95485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "###############################\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(Xtrain, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "3nsJ6kc8wHYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Load test data and create text sequences of max document length**"
      ]
    },
    {
      "metadata": {
        "id": "eAPI2X55wH-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# process all testing Questions\n",
        "test_docs = process_docs(X_test, vocab)\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(test_docs)\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-x5Ic6iPwl8d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test predictions**"
      ]
    },
    {
      "metadata": {
        "id": "UpQP_K_-jjvQ",
        "colab_type": "code",
        "outputId": "443e6da4-a313-4f5d-fa91-cc179679a1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred= (model.predict(Xtest))\n",
        "sc1 = accuracy_score(y_test, y_pred)\n",
        "print(\"Acc=\",sc1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc= 0.6381748862378551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O_YPdkh7wxtF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "metadata": {
        "id": "a7YdxyhXwyYA",
        "colab_type": "code",
        "outputId": "52f29cc7-40b2-4654-8269-a961d9184151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "\n",
        "print(\"tn =\",tn, \" fp=\",fp, \" fn=\",fn, \"tp=\",tp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tn = 18556  fp= 6003  fn= 11649 tp= 12578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GjA5l2now-HA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Percision, Recall and F Measure**"
      ]
    },
    {
      "metadata": {
        "id": "wMvW4Eo2w-qu",
        "colab_type": "code",
        "outputId": "5c1f44d5-328e-4550-f7ca-4a9e36f3f1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "P = tp/(tp+fn)\n",
        "R= tp/(tp+fp)\n",
        "F1=2*P*R/(P+R)\n",
        "print(\"precission=\",P,\"recall=\",R,\" F1=\",F1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precission= 0.5191728237090849 recall= 0.6769280447769227  F1= 0.5876471687535041\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}